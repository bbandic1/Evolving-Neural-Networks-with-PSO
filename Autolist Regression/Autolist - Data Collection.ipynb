{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ebf282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Benjo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2071958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading links from autolist_car_links.json...\n",
      "Loaded 104399 links, 104399 unique links to process.\n",
      "No existing data file found or file is empty/invalid. Starting fresh.\n",
      "\n",
      "Processing ad 1/104399: https://www.autolist.com/listings#door_count[]=2&driveline[]=4X4&exclude_no_price=true&exclude_regional=true&limit=20&page=1&radius=Any&vin=1C4AJWAG5GL335490\n",
      "  - Error scraping details for https://www.autolist.com/listings#door_count[]=2&driveline[]=4X4&exclude_no_price=true&exclude_regional=true&limit=20&page=1&radius=Any&vin=1C4AJWAG5GL335490: name 'ACTION_DELAY' is not defined\n",
      "\n",
      "Processing ad 2/104399: https://www.autolist.com/listings#door_count[]=2&driveline[]=4X4&exclude_no_price=true&exclude_regional=true&limit=20&page=1&radius=Any&vin=1C4AJWAGXEL327317\n",
      "  - Error scraping details for https://www.autolist.com/listings#door_count[]=2&driveline[]=4X4&exclude_no_price=true&exclude_regional=true&limit=20&page=1&radius=Any&vin=1C4AJWAGXEL327317: name 'ACTION_DELAY' is not defined\n",
      "\n",
      "Scraping process interrupted by user.\n",
      "WebDriver closed.\n",
      "\n",
      "--- Individual Ad Scraping Finished (or Interrupted) ---\n",
      "No car data was successfully scraped in this session.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import math # For math.isnan\n",
    "\n",
    "# --- Configuration ---\n",
    "LINKS_JSON_FILE_PATH = 'autolist_car_links.json'\n",
    "OUTPUT_DATA_JSON_PATH = 'autolist_car_details.json'\n",
    "\n",
    "REQUEST_DELAY_DETAIL_PAGE = 2.5\n",
    "MAX_ADS_TO_SCRAPE = 10\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def clean_price(price_str):\n",
    "    if price_str is None: return None\n",
    "    price_str = price_str.replace('$', '').replace(',', '').strip()\n",
    "    try:\n",
    "        return int(float(price_str))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def clean_mileage(mileage_str):\n",
    "    if mileage_str is None: return None\n",
    "    mileage_str = mileage_str.lower().replace('miles', '').replace(',', '').strip()\n",
    "    try:\n",
    "        return int(float(mileage_str))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def extract_monthly_payment(payment_str):\n",
    "    if payment_str is None: return None\n",
    "    match = re.search(r'\\$(\\d{1,3}(?:,\\d{3})*(\\.\\d+)?)/mo', payment_str)\n",
    "    if match:\n",
    "        try:\n",
    "            return int(float(match.group(1).replace(',', '')))\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip() if text else None\n",
    "\n",
    "# --- WebDriver Setup ---\n",
    "def web_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--headless') \n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument(\"--window-size=1280,1024\")\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\")\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# --- Main Scraping Logic ---\n",
    "all_scraped_car_data = []\n",
    "processed_links = set() \n",
    "driver = None\n",
    "\n",
    "try:\n",
    "    # Load all collected links\n",
    "    print(f\"Loading links from {LINKS_JSON_FILE_PATH}...\")\n",
    "    try:\n",
    "        with open(LINKS_JSON_FILE_PATH, 'r') as f:\n",
    "            all_links_from_file = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Links file {LINKS_JSON_FILE_PATH} not found. Please run Phase 1 first.\")\n",
    "        sys.exit()\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERROR: Could not decode JSON from {LINKS_JSON_FILE_PATH}. File might be corrupted.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Deduplicate links (though they should be unique if saved from a set)\n",
    "    unique_ad_links = sorted(list(set(all_links_from_file)))\n",
    "    print(f\"Loaded {len(all_links_from_file)} links, {len(unique_ad_links)} unique links to process.\")\n",
    "\n",
    "    if not unique_ad_links:\n",
    "        print(\"No links to process. Exiting.\")\n",
    "        sys.exit()\n",
    "\n",
    "    driver = web_driver()\n",
    "    wait = WebDriverWait(driver, 20) # Wait up to 20 seconds for elements\n",
    "\n",
    "    # --- Optionally load previously scraped data to resume ---\n",
    "    try:\n",
    "        df_existing = pd.read_json(OUTPUT_DATA_JSON_PATH, orient='records', lines=False) # Assuming one JSON array\n",
    "        all_scraped_car_data = df_existing.to_dict('records')\n",
    "        for record in all_scraped_car_data:\n",
    "            if 'Scraped_URL' in record: # Assuming you add the URL as a field\n",
    "                 processed_links.add(record['Scraped_URL'])\n",
    "        print(f\"Loaded {len(all_scraped_car_data)} previously scraped car details. Already processed {len(processed_links)} links.\")\n",
    "    except (FileNotFoundError, ValueError): # ValueError for empty/invalid JSON\n",
    "        print(\"No existing data file found or file is empty/invalid. Starting fresh.\")\n",
    "        all_scraped_car_data = []\n",
    "        processed_links = set()\n",
    "\n",
    "\n",
    "    links_to_scrape_count = 0\n",
    "    for i, ad_url in enumerate(unique_ad_links):\n",
    "        if MAX_ADS_TO_SCRAPE is not None and links_to_scrape_count >= MAX_ADS_TO_SCRAPE:\n",
    "            print(f\"Reached MAX_ADS_TO_SCRAPE limit of {MAX_ADS_TO_SCRAPE}.\")\n",
    "            break\n",
    "\n",
    "        if ad_url in processed_links:\n",
    "            # print(f\"Skipping already processed link: {ad_url}\")\n",
    "            continue # Skip if already processed in a previous run\n",
    "\n",
    "        print(f\"\\nProcessing ad {i+1}/{len(unique_ad_links)}: {ad_url}\")\n",
    "        sys.stdout.flush()\n",
    "        car_details = {'Scraped_URL': ad_url} # Add the URL itself for tracking\n",
    "\n",
    "        try:\n",
    "            driver.get(ad_url)\n",
    "\n",
    "            wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"vdp-scroll-container\"]/div/div[2]/div[1]/div[1]/div[1]/div[3]/div/div[1]/div[1]'))) # Wait for Name\n",
    "            time.sleep(ACTION_DELAY) # Give some time for JS to populate everything\n",
    "\n",
    "            # --- Name ---\n",
    "            try:\n",
    "                name_element = driver.find_element(By.XPATH, '//*[@id=\"vdp-scroll-container\"]/div/div[2]/div[1]/div[1]/div[1]/div[3]/div/div[1]/div[1]')\n",
    "                car_details['Name'] = clean_text(name_element.text)\n",
    "            except NoSuchElementException:\n",
    "                car_details['Name'] = None\n",
    "                print(\"  - Name not found\")\n",
    "\n",
    "            # --- Price ---\n",
    "            try:\n",
    "                price_element = driver.find_element(By.XPATH, '//*[@id=\"vdp-scroll-container\"]/div/div[2]/div[1]/div[1]/div[1]/div[3]/div/div[1]/div[2]/div[1]')\n",
    "                car_details['Price'] = clean_price(price_element.text)\n",
    "            except NoSuchElementException:\n",
    "                car_details['Price'] = None\n",
    "                print(\"  - Price not found\")\n",
    "\n",
    "            # --- PricePerMonth ---\n",
    "            try:\n",
    "                price_per_month_element = driver.find_element(By.XPATH, '//*[@id=\"vdp-scroll-container\"]/div/div[2]/div[1]/div[1]/div[1]/div[3]/div/div[1]/div[2]/div[2]')\n",
    "                car_details['PricePerMonth'] = extract_monthly_payment(price_per_month_element.text)\n",
    "            except NoSuchElementException:\n",
    "                car_details['PricePerMonth'] = None\n",
    "\n",
    "            # --- Location (from top summary) ---\n",
    "            try:\n",
    "                location_element = driver.find_element(By.XPATH, '//*[@id=\"vdp-scroll-container\"]/div/div[2]/div[1]/div[1]/div[1]/div[3]/div/div[2]/div[3]')\n",
    "                car_details['Location_Summary'] = clean_text(location_element.text)\n",
    "            except NoSuchElementException:\n",
    "                car_details['Location_Summary'] = None\n",
    "\n",
    "            # --- Days on Market & Price Comparison (from top summary) ---\n",
    "            try:\n",
    "                price_comp_element = driver.find_element(By.CSS_SELECTOR, 'div.jsx-b0d76f52871971e9.price-comparison-text')\n",
    "                car_details['Price_Comparison_Text'] = clean_text(price_comp_element.text)\n",
    "            except: car_details['Price_Comparison_Text'] = None\n",
    "\n",
    "            try:\n",
    "                days_market_element = driver.find_element(By.CSS_SELECTOR, 'div.jsx-b0d76f52871971e9.time-on-market')\n",
    "                car_details['Days_On_Market'] = clean_text(days_market_element.text) # Will be like \"6\"\n",
    "            except: car_details['Days_On_Market'] = None\n",
    "\n",
    "\n",
    "            page_content_for_bs = driver.page_source\n",
    "            soup_detail = BeautifulSoup(page_content_for_bs, 'html.parser')\n",
    "\n",
    "            vehicle_info_dl = soup_detail.find('dl', class_='property-list')\n",
    "            if vehicle_info_dl:\n",
    "                dts = vehicle_info_dl.find_all('dt')\n",
    "                dds = vehicle_info_dl.find_all('dd')\n",
    "\n",
    "                for dt, dd in zip(dts, dds):\n",
    "                    spec_name = clean_text(dt.text.replace(':', ''))\n",
    "                    spec_value = clean_text(dd.text)\n",
    "\n",
    "                    if spec_name == \"Mileage\":\n",
    "                        car_details['Mileage'] = clean_mileage(spec_value)\n",
    "                    elif spec_name == \"Trim\":\n",
    "                        car_details['Trim'] = spec_value\n",
    "                    elif spec_name == \"Engine\":\n",
    "                        car_details['Engine'] = spec_value\n",
    "                    elif spec_name == \"VIN\":\n",
    "                        car_details['VIN'] = spec_value\n",
    "                    elif spec_name == \"Exterior color\":\n",
    "                        car_details['Exterior_Color'] = spec_value\n",
    "                    elif spec_name == \"Condition\":\n",
    "                        car_details['Condition'] = spec_value\n",
    "                    elif spec_name == \"Combined gas mileage\": # Renamed to avoid space\n",
    "                        car_details['Gas_Mileage_Combined'] = spec_value\n",
    "                    elif spec_name == \"Doors\":\n",
    "                        car_details['Doors'] = spec_value\n",
    "                    elif spec_name == \"Transmission\":\n",
    "                        car_details['Transmission'] = spec_value\n",
    "                    elif spec_name == \"Drivetrain\":\n",
    "                        car_details['Drivetrain'] = spec_value\n",
    "                    elif spec_name == \"Fuel type\":\n",
    "                        car_details['Fuel_Type'] = spec_value\n",
    "                    elif spec_name == \"Interior color\":\n",
    "                        car_details['Interior_Color'] = spec_value\n",
    "                    elif spec_name == \"Body type\": # Was Body Style\n",
    "                        car_details['Body_Style'] = spec_value\n",
    "                    elif spec_name == \"Stock #\": # Was Stock#\n",
    "                        car_details['Stock_Number'] = spec_value\n",
    "                    # Add more specific elif for other known dt texts if needed\n",
    "            else:\n",
    "                print(\"  - Could not find <dl class='property-list'> for detailed specs.\")\n",
    "\n",
    "\n",
    "            # --- Key Features (Interior, Exterior, Safety, Other) ---\n",
    "            # These are usually in <ul> elements following specific <h2> or <div> titles\n",
    "            feature_sections_map = {\n",
    "                \"Interior\": \"Interior_Features\",\n",
    "                \"Exterior\": \"Exterior_Features\",\n",
    "                \"Safety\": \"Safety_Features\",\n",
    "                \"Other\": \"Other_Features\"\n",
    "            }\n",
    "            \n",
    "            all_feature_titles = soup_detail.find_all('div', class_=lambda x: x and x.startswith('title') and x.endswith('jsx-'))\n",
    "\n",
    "            for title_div in all_feature_titles:\n",
    "                section_title_text = clean_text(title_div.text)\n",
    "                if section_title_text in feature_sections_map:\n",
    "                    ul_features = title_div.find_next_sibling('ul', attrs={'data-testid': 'feature-list'})\n",
    "                    if ul_features:\n",
    "                        features_list = [clean_text(li.text) for li in ul_features.find_all('li', class_='feature-list-item')]\n",
    "                        car_details[feature_sections_map[section_title_text]] = features_list\n",
    "                    else:\n",
    "                         car_details[feature_sections_map[section_title_text]] = []\n",
    "                elif section_title_text == \"Vehicle Information\": # To ensure we got all from dl\n",
    "                    pass # Already handled by dl.property-list\n",
    "\n",
    "            all_scraped_car_data.append(car_details)\n",
    "            processed_links.add(ad_url) # Mark as processed\n",
    "            links_to_scrape_count += 1\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"  - Timeout waiting for details on page: {ad_url}\")\n",
    "        except Exception as e_detail:\n",
    "            print(f\"  - Error scraping details for {ad_url}: {e_detail}\")\n",
    "\n",
    "        # Save progress periodically\n",
    "        if links_to_scrape_count > 0 and links_to_scrape_count % 50 == 0: # Save every 50 cars\n",
    "            temp_df = pd.DataFrame(all_scraped_car_data)\n",
    "            save_links_to_json(temp_df.to_dict('records'), OUTPUT_DATA_JSON_PATH) # Save all data processed so far\n",
    "            print(f\"--- Progress saved after {links_to_scrape_count} cars ---\")\n",
    "\n",
    "        time.sleep(REQUEST_DELAY_DETAIL_PAGE)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nScraping process interrupted by user.\")\n",
    "except Exception as e_main:\n",
    "    print(f\"A critical error occurred in the main scraping process: {e_main}\")\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"WebDriver closed.\")\n",
    "\n",
    "    print(\"\\n--- Individual Ad Scraping Finished (or Interrupted) ---\")\n",
    "    if all_scraped_car_data:\n",
    "        final_df = pd.DataFrame(all_scraped_car_data)\n",
    "        print(f\"Total cars scraped: {len(final_df)}\")\n",
    "        print(\"\\nSample of scraped data:\")\n",
    "        print(final_df.head().to_string())\n",
    "\n",
    "        # Save final data to JSON\n",
    "        print(f\"\\nSaving final data to {OUTPUT_DATA_JSON_PATH}...\")\n",
    "        save_links_to_json(final_df.to_dict('records'), OUTPUT_DATA_JSON_PATH) # Overwrites with full data\n",
    "\n",
    "        print(\"All data saved.\")\n",
    "    else:\n",
    "        print(\"No car data was successfully scraped in this session.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
