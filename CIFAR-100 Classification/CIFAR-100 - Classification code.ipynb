{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a7c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee4078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-100 dataset...\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n",
      "Dataset loaded successfully.\n",
      "\n",
      "--- Data Inspection and Preprocessing ---\n",
      "Training data shape: (50000, 32, 32, 3)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Input shape for model: (32, 32, 3)\n",
      "Number of classes: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- LOAD DATA ---\n",
    "print(\"Loading CIFAR-100 dataset...\")\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar100.load_data()\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# --- DATA PREPROCESSING ---\n",
    "# Scale pixel values from 0-255 to 0-1\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Define constants\n",
    "INPUT_SHAPE = X_train_full.shape[1:]\n",
    "NUM_CLASSES = len(np.unique(y_train_full))\n",
    "\n",
    "print(\"\\n--- Data Inspection and Preprocessing ---\")\n",
    "print(f\"Training data shape: {X_train_full.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Input shape for model: {INPUT_SHAPE}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(X_train_full[i])\n",
    "    plt.title(f\"Label: {y_train_full[i][0]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdb9104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Baseline CNN Model for CIFAR-100 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,300</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m51,300\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,391,236</span> (5.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,391,236\u001b[0m (5.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,389,316</span> (5.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,389,316\u001b[0m (5.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> (7.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,920\u001b[0m (7.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Baseline Model ---\n",
      "Epoch 1/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 33ms/step - accuracy: 0.0692 - loss: 4.6745 - val_accuracy: 0.2026 - val_loss: 3.3838\n",
      "Epoch 2/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.2028 - loss: 3.3694 - val_accuracy: 0.2663 - val_loss: 3.0826\n",
      "Epoch 3/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.2881 - loss: 2.8543 - val_accuracy: 0.3593 - val_loss: 2.5151\n",
      "Epoch 4/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.3379 - loss: 2.5813 - val_accuracy: 0.3988 - val_loss: 2.3086\n",
      "Epoch 5/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.3743 - loss: 2.3928 - val_accuracy: 0.4178 - val_loss: 2.2434\n",
      "Epoch 6/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.4087 - loss: 2.2399 - val_accuracy: 0.4306 - val_loss: 2.1983\n",
      "Epoch 7/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4367 - loss: 2.1047 - val_accuracy: 0.4518 - val_loss: 2.0859\n",
      "Epoch 8/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4569 - loss: 2.0076 - val_accuracy: 0.4550 - val_loss: 2.0941\n",
      "Epoch 9/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4763 - loss: 1.9174 - val_accuracy: 0.4650 - val_loss: 2.0260\n",
      "Epoch 10/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.5022 - loss: 1.8069 - val_accuracy: 0.4951 - val_loss: 1.8878\n",
      "Epoch 11/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.5160 - loss: 1.7395 - val_accuracy: 0.4948 - val_loss: 1.9105\n",
      "Epoch 12/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5331 - loss: 1.6700 - val_accuracy: 0.5193 - val_loss: 1.8198\n",
      "Epoch 13/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5441 - loss: 1.6013 - val_accuracy: 0.4919 - val_loss: 1.9639\n",
      "Epoch 14/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5630 - loss: 1.5428 - val_accuracy: 0.5354 - val_loss: 1.7480\n",
      "Epoch 15/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5704 - loss: 1.4937 - val_accuracy: 0.5255 - val_loss: 1.7979\n",
      "Epoch 16/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.5847 - loss: 1.4326 - val_accuracy: 0.5250 - val_loss: 1.8136\n",
      "Epoch 17/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5957 - loss: 1.3973 - val_accuracy: 0.5342 - val_loss: 1.7602\n",
      "Epoch 18/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6072 - loss: 1.3521 - val_accuracy: 0.5195 - val_loss: 1.8167\n",
      "Epoch 19/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6168 - loss: 1.3114 - val_accuracy: 0.5288 - val_loss: 1.8056\n",
      "Epoch 20/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.6215 - loss: 1.2768 - val_accuracy: 0.5541 - val_loss: 1.7057\n",
      "Epoch 21/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6332 - loss: 1.2472 - val_accuracy: 0.5417 - val_loss: 1.7555\n",
      "Epoch 22/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6390 - loss: 1.2125 - val_accuracy: 0.5370 - val_loss: 1.7714\n",
      "Epoch 23/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6460 - loss: 1.1906 - val_accuracy: 0.5444 - val_loss: 1.7847\n",
      "Epoch 24/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6543 - loss: 1.1596 - val_accuracy: 0.5440 - val_loss: 1.8029\n",
      "Epoch 25/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6676 - loss: 1.1171 - val_accuracy: 0.5575 - val_loss: 1.7082\n",
      "Epoch 26/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6713 - loss: 1.0947 - val_accuracy: 0.5537 - val_loss: 1.7643\n",
      "Epoch 27/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 36ms/step - accuracy: 0.6774 - loss: 1.0688 - val_accuracy: 0.5314 - val_loss: 1.8775\n",
      "Epoch 28/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.6852 - loss: 1.0458 - val_accuracy: 0.5578 - val_loss: 1.7430\n",
      "Epoch 29/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6885 - loss: 1.0226 - val_accuracy: 0.5628 - val_loss: 1.7422\n",
      "Epoch 30/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - accuracy: 0.6880 - loss: 1.0141 - val_accuracy: 0.5504 - val_loss: 1.7740\n",
      "Epoch 31/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 34ms/step - accuracy: 0.6958 - loss: 1.0051 - val_accuracy: 0.5551 - val_loss: 1.7775\n",
      "Epoch 32/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - accuracy: 0.7061 - loss: 0.9685 - val_accuracy: 0.5588 - val_loss: 1.7554\n",
      "Epoch 33/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.7022 - loss: 0.9677 - val_accuracy: 0.5379 - val_loss: 1.8789\n",
      "Epoch 34/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.7112 - loss: 0.9448 - val_accuracy: 0.5672 - val_loss: 1.7212\n",
      "Epoch 35/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.7163 - loss: 0.9287 - val_accuracy: 0.5742 - val_loss: 1.7036\n",
      "Epoch 36/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.7205 - loss: 0.9082 - val_accuracy: 0.5657 - val_loss: 1.7116\n",
      "Epoch 37/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 34ms/step - accuracy: 0.7219 - loss: 0.8998 - val_accuracy: 0.5727 - val_loss: 1.7134\n",
      "Epoch 38/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.7245 - loss: 0.8897 - val_accuracy: 0.5707 - val_loss: 1.7352\n",
      "Epoch 39/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7280 - loss: 0.8793 - val_accuracy: 0.5669 - val_loss: 1.7476\n",
      "Epoch 40/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7302 - loss: 0.8625 - val_accuracy: 0.5706 - val_loss: 1.7209\n",
      "Epoch 41/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7332 - loss: 0.8557 - val_accuracy: 0.5702 - val_loss: 1.7551\n",
      "Epoch 42/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7389 - loss: 0.8346 - val_accuracy: 0.5711 - val_loss: 1.7573\n",
      "Epoch 43/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7387 - loss: 0.8339 - val_accuracy: 0.5743 - val_loss: 1.7199\n",
      "Epoch 44/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7474 - loss: 0.8049 - val_accuracy: 0.5698 - val_loss: 1.7493\n",
      "Epoch 45/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7464 - loss: 0.8134 - val_accuracy: 0.5806 - val_loss: 1.7337\n",
      "Epoch 46/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.7554 - loss: 0.7863 - val_accuracy: 0.5723 - val_loss: 1.7454\n",
      "Epoch 47/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7487 - loss: 0.7990 - val_accuracy: 0.5763 - val_loss: 1.7411\n",
      "Epoch 48/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7542 - loss: 0.7780 - val_accuracy: 0.5834 - val_loss: 1.7216\n",
      "Epoch 49/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7542 - loss: 0.7709 - val_accuracy: 0.5814 - val_loss: 1.7333\n",
      "Epoch 50/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7672 - loss: 0.7433 - val_accuracy: 0.5741 - val_loss: 1.7833\n",
      "Epoch 51/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7639 - loss: 0.7587 - val_accuracy: 0.5788 - val_loss: 1.7689\n",
      "Epoch 52/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.7674 - loss: 0.7380 - val_accuracy: 0.5806 - val_loss: 1.7442\n",
      "Epoch 53/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.7663 - loss: 0.7424 - val_accuracy: 0.5768 - val_loss: 1.7382\n",
      "Epoch 54/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.7758 - loss: 0.7139 - val_accuracy: 0.5745 - val_loss: 1.7668\n",
      "Epoch 55/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.7729 - loss: 0.7244 - val_accuracy: 0.5830 - val_loss: 1.7403\n",
      "Epoch 56/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7717 - loss: 0.7162 - val_accuracy: 0.5828 - val_loss: 1.7684\n",
      "Epoch 57/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7754 - loss: 0.7187 - val_accuracy: 0.5799 - val_loss: 1.7463\n",
      "Epoch 58/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.7803 - loss: 0.6946 - val_accuracy: 0.5793 - val_loss: 1.7458\n",
      "Epoch 59/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7804 - loss: 0.6964 - val_accuracy: 0.5857 - val_loss: 1.7467\n",
      "Epoch 60/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7827 - loss: 0.6802 - val_accuracy: 0.5834 - val_loss: 1.7604\n",
      "Epoch 61/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7866 - loss: 0.6752 - val_accuracy: 0.5840 - val_loss: 1.7502\n",
      "Epoch 62/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7916 - loss: 0.6549 - val_accuracy: 0.5876 - val_loss: 1.7548\n",
      "Epoch 63/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7834 - loss: 0.6811 - val_accuracy: 0.5833 - val_loss: 1.7504\n",
      "Epoch 64/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7886 - loss: 0.6743 - val_accuracy: 0.5763 - val_loss: 1.7681\n",
      "Epoch 65/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7901 - loss: 0.6614 - val_accuracy: 0.5774 - val_loss: 1.8176\n",
      "Epoch 66/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7912 - loss: 0.6522 - val_accuracy: 0.5796 - val_loss: 1.7840\n",
      "Epoch 67/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.7968 - loss: 0.6419 - val_accuracy: 0.5878 - val_loss: 1.7389\n",
      "Epoch 68/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.8013 - loss: 0.6271 - val_accuracy: 0.5729 - val_loss: 1.8198\n",
      "Epoch 69/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.8002 - loss: 0.6344 - val_accuracy: 0.5925 - val_loss: 1.7594\n",
      "Epoch 70/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.8001 - loss: 0.6279 - val_accuracy: 0.5848 - val_loss: 1.7565\n",
      "Epoch 71/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.7991 - loss: 0.6306 - val_accuracy: 0.5813 - val_loss: 1.7802\n",
      "Epoch 72/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8003 - loss: 0.6291 - val_accuracy: 0.5862 - val_loss: 1.7911\n",
      "Epoch 73/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8012 - loss: 0.6139 - val_accuracy: 0.5923 - val_loss: 1.7547\n",
      "Epoch 74/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8068 - loss: 0.6016 - val_accuracy: 0.5862 - val_loss: 1.7948\n",
      "Epoch 75/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8050 - loss: 0.6072 - val_accuracy: 0.5706 - val_loss: 1.8578\n",
      "Epoch 76/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8092 - loss: 0.6018 - val_accuracy: 0.5803 - val_loss: 1.8034\n",
      "Epoch 77/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8062 - loss: 0.5981 - val_accuracy: 0.5838 - val_loss: 1.7930\n",
      "Epoch 78/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8173 - loss: 0.5760 - val_accuracy: 0.5860 - val_loss: 1.7840\n",
      "Epoch 79/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8111 - loss: 0.5884 - val_accuracy: 0.5921 - val_loss: 1.7551\n",
      "Epoch 80/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8101 - loss: 0.5912 - val_accuracy: 0.5795 - val_loss: 1.8210\n",
      "Epoch 81/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8131 - loss: 0.5798 - val_accuracy: 0.5885 - val_loss: 1.7690\n",
      "Epoch 82/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8133 - loss: 0.5842 - val_accuracy: 0.5868 - val_loss: 1.7872\n",
      "Epoch 83/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.8188 - loss: 0.5699 - val_accuracy: 0.5830 - val_loss: 1.8161\n",
      "Epoch 84/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8127 - loss: 0.5773 - val_accuracy: 0.5853 - val_loss: 1.7767\n",
      "Epoch 84: early stopping\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "\n",
      "--- Evaluating Baseline Model on Test Set ---\n",
      "\n",
      "Baseline Model Test Loss: 1.7295\n",
      "Baseline Model Test Accuracy: 0.5897\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "\n",
      "Baseline Model Classification Report (Top 5 classes and summary):\n",
      "Macro Avg F1-score: 0.5868\n",
      "Weighted Avg F1-score: 0.5868\n",
      "Baseline Matthews Correlation Coefficient (MCC): 0.5857\n",
      "Baseline Cohen's Kappa: 0.5856\n",
      "Baseline Balanced Accuracy: 0.5897\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the Baseline CNN Model\n",
    "print(\"--- Building Baseline CNN Model for CIFAR-100 ---\")\n",
    "\n",
    "baseline_model = Sequential([\n",
    "    Input(shape=INPUT_SHAPE),\n",
    "    \n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "baseline_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "baseline_model.summary()\n",
    "\n",
    "# Train the Baseline Model\n",
    "print(\"\\n--- Training Baseline Model ---\")\n",
    "baseline_early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=15,        \n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train_full, y_train_full,\n",
    "    epochs=100,\n",
    "    batch_size=64, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[baseline_early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Baseline Model\n",
    "print(\"\\n--- Evaluating Baseline Model on Test Set ---\")\n",
    "loss_baseline, accuracy_baseline = baseline_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nBaseline Model Test Loss: {loss_baseline:.4f}\")\n",
    "print(f\"Baseline Model Test Accuracy: {accuracy_baseline:.4f}\")\n",
    "\n",
    "# Get full metrics\n",
    "y_pred_probs_baseline = baseline_model.predict(X_test)\n",
    "y_pred_baseline = np.argmax(y_pred_probs_baseline, axis=1)\n",
    "\n",
    "print(\"\\nBaseline Model Classification Report (Top 5 classes and summary):\")\n",
    "baseline_report_dict = classification_report(y_test, y_pred_baseline, output_dict=True)\n",
    "print(f\"Macro Avg F1-score: {baseline_report_dict['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted Avg F1-score: {baseline_report_dict['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "\n",
    "# Store all metrics for final comparison\n",
    "baseline_mcc = matthews_corrcoef(y_test, y_pred_baseline)\n",
    "baseline_cohen_kappa = cohen_kappa_score(y_test, y_pred_baseline)\n",
    "baseline_balanced_accuracy = balanced_accuracy_score(y_test, y_pred_baseline)\n",
    "baseline_macro_f1 = baseline_report_dict['macro avg']['f1-score']\n",
    "baseline_weighted_f1 = baseline_report_dict['weighted avg']['f1-score']\n",
    "\n",
    "print(f\"Baseline Matthews Correlation Coefficient (MCC): {baseline_mcc:.4f}\")\n",
    "print(f\"Baseline Cohen's Kappa: {baseline_cohen_kappa:.4f}\")\n",
    "print(f\"Baseline Balanced Accuracy: {baseline_balanced_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45de0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 14:54:20,934 - pyswarms.single.global_best - INFO - Optimize for 5 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PSO Optimization Setup for CIFAR-100 ---\n",
      "PSO training set size: (40000, 32, 32, 3)\n",
      "PSO validation set size: (10000, 32, 32, 3)\n",
      "\n",
      "Starting PSO with 5 particles for 5 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   0%|          |0/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PSO Eval 1: F1=51, F2=75, F3=172, Dense=496, LR=0.00730 -> Best Val Acc: 0.5561\n",
      "\n",
      "PSO Eval 2: F1=35, F2=96, F3=199, Dense=674, LR=0.00545 -> Best Val Acc: 0.5579\n",
      "\n",
      "PSO Eval 3: F1=26, F2=51, F3=217, Dense=388, LR=0.00659 -> Best Val Acc: 0.5457\n",
      "\n",
      "PSO Eval 4: F1=30, F2=46, F3=142, Dense=406, LR=0.00251 -> Best Val Acc: 0.5671\n",
      "\n",
      "PSO Eval 5: F1=44, F2=56, F3=208, Dense=773, LR=0.00698"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  20%|██        |1/5, best_cost=0.433"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Best Val Acc: 0.5471\n",
      "\n",
      "PSO Eval 6: F1=47, F2=71, F3=164, Dense=483, LR=0.00756 -> Best Val Acc: 0.5646\n",
      "\n",
      "PSO Eval 7: F1=35, F2=93, F3=200, Dense=603, LR=0.00244 -> Best Val Acc: 0.5737\n",
      "\n",
      "PSO Eval 8: F1=27, F2=50, F3=211, Dense=390, LR=0.00170 -> Best Val Acc: 0.5649\n",
      "\n",
      "PSO Eval 9: F1=31, F2=47, F3=142, Dense=407, LR=0.00045 -> Best Val Acc: 0.5526\n",
      "\n",
      "PSO Eval 10: F1=43, F2=55, F3=189, Dense=758, LR=0.00714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  40%|████      |2/5, best_cost=0.426"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Best Val Acc: 0.5348\n",
      "\n",
      "PSO Eval 11: F1=41, F2=69, F3=165, Dense=500, LR=0.00918 -> Best Val Acc: 0.5434\n",
      "\n",
      "PSO Eval 12: F1=35, F2=90, F3=201, Dense=539, LR=0.00568 -> Best Val Acc: 0.5503\n",
      "\n",
      "PSO Eval 13: F1=30, F2=55, F3=205, Dense=447, LR=0.00126 -> Best Val Acc: 0.5705\n",
      "\n",
      "PSO Eval 14: F1=32, F2=48, F3=151, Dense=458, LR=0.00265 -> Best Val Acc: 0.5658\n",
      "\n",
      "PSO Eval 15: F1=40, F2=56, F3=175, Dense=710, LR=0.00034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  60%|██████    |3/5, best_cost=0.426"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Best Val Acc: 0.5615\n",
      "\n",
      "PSO Eval 16: F1=35, F2=67, F3=174, Dense=513, LR=0.00419 -> Best Val Acc: 0.5580\n",
      "\n",
      "PSO Eval 17: F1=35, F2=88, F3=201, Dense=499, LR=0.00844 -> Best Val Acc: 0.5600\n",
      "\n",
      "PSO Eval 18: F1=33, F2=66, F3=199, Dense=502, LR=0.00118 -> Best Val Acc: 0.5815\n",
      "\n",
      "PSO Eval 19: F1=33, F2=57, F3=172, Dense=503, LR=0.00956 -> Best Val Acc: 0.5377\n",
      "\n",
      "PSO Eval 20: F1=36, F2=62, F3=165, Dense=646, LR=0.00360"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  80%|████████  |4/5, best_cost=0.419"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Best Val Acc: 0.5633\n",
      "\n",
      "PSO Eval 21: F1=30, F2=66, F3=182, Dense=518, LR=0.00546 -> Best Val Acc: 0.5478\n",
      "\n",
      "PSO Eval 22: F1=35, F2=85, F3=200, Dense=475, LR=0.00219 -> Best Val Acc: 0.5653\n",
      "\n",
      "PSO Eval 23: F1=36, F2=77, F3=193, Dense=551, LR=0.00606 -> Best Val Acc: 0.5628\n",
      "\n",
      "PSO Eval 24: F1=34, F2=65, F3=178, Dense=534, LR=0.00144 -> Best Val Acc: 0.5785\n",
      "\n",
      "PSO Eval 25: F1=33, F2=68, F3=164, Dense=558, LR=0.00096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████|5/5, best_cost=0.419\n",
      "2025-06-15 21:22:58,778 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.41850000619888306, best pos: [3.31375080e+01 6.63929291e+01 1.98950130e+02 5.01942790e+02\n",
      " 1.17825716e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Best Val Acc: 0.5683\n",
      "\n",
      "--- PSO Optimization Finished ---\n",
      "Best cost (1 - val_accuracy) from PSO: 0.4185\n",
      "This implies a best validation accuracy of: 0.5815\n",
      "\n",
      "Formatted Best Hyperparameters from PSO:\n",
      "  Filters: Block1=33, Block2=66, Block3=199\n",
      "  Dense Units: 502\n",
      "  Learning Rate: 0.001178\n"
     ]
    }
   ],
   "source": [
    "# Split training data for PSO's internal validation\n",
    "X_train_pso, X_val_pso, y_train_pso, y_val_pso = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=123, stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(\"--- PSO Optimization Setup for CIFAR-100 ---\")\n",
    "print(f\"PSO training set size: {X_train_pso.shape}\")\n",
    "print(f\"PSO validation set size: {X_val_pso.shape}\")\n",
    "\n",
    "\n",
    "# Define Hyperparameter Search Space for the CNN\n",
    "min_bounds = np.array([16, 32, 64,  128, 0.0001])\n",
    "max_bounds = np.array([64, 128, 256, 1024, 0.01])\n",
    "bounds_pso = (min_bounds, max_bounds)\n",
    "n_dimensions = len(min_bounds)\n",
    "\n",
    "# Define PSO Objective Function\n",
    "pso_iteration_count = 0\n",
    "def cnn_fitness_function_cifar(params):\n",
    "    global pso_iteration_count\n",
    "    pso_iteration_count += 1\n",
    "\n",
    "    # Unpack hyperparameters\n",
    "    f1 = int(round(params[0]))\n",
    "    f2 = int(round(params[1]))\n",
    "    f3 = int(round(params[2]))\n",
    "    d1 = int(round(params[3]))\n",
    "    lr = float(params[4])\n",
    "\n",
    "    print(f\"\\nPSO Eval {pso_iteration_count}: F1={f1}, F2={f2}, F3={f3}, Dense={d1}, LR={lr:.5f}\", end=\"\")\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42 + pso_iteration_count)\n",
    "    np.random.seed(42 + pso_iteration_count)\n",
    "\n",
    "    model_pso = Sequential([\n",
    "        Input(shape=INPUT_SHAPE),\n",
    "        Conv2D(f1, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "        Conv2D(f1, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)), Dropout(0.2),\n",
    "        \n",
    "        Conv2D(f2, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "        Conv2D(f2, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)), Dropout(0.3),\n",
    "        \n",
    "        Conv2D(f3, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "        Conv2D(f3, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)), Dropout(0.4),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(d1, activation='relu'), BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model_pso.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # More aggressive early stopping for the quick PSO evaluation\n",
    "    early_stop_pso = EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    history_pso = model_pso.fit(\n",
    "        X_train_pso, y_train_pso,\n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_pso, y_val_pso),\n",
    "        callbacks=[early_stop_pso],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_accuracy = np.max(history_pso.history.get('val_accuracy', [0]))\n",
    "    print(f\" -> Best Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return 1.0 - val_accuracy\n",
    "\n",
    "def pso_objective_wrapper(particles_batch):\n",
    "    return np.array([cnn_fitness_function_cifar(p) for p in particles_batch])\n",
    "\n",
    "# Configure and Run PSO\n",
    "n_particles_pso = 5\n",
    "pso_iters = 5\n",
    "\n",
    "options_pso = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "print(f\"\\nStarting PSO with {n_particles_pso} particles for {pso_iters} iterations.\")\n",
    "\n",
    "pso_optimizer = ps.single.GlobalBestPSO(\n",
    "    n_particles=n_particles_pso, dimensions=n_dimensions, options=options_pso, bounds=bounds_pso\n",
    ")\n",
    "\n",
    "best_cost_pso, best_params_pso = pso_optimizer.optimize(pso_objective_wrapper, iters=pso_iters, verbose=True)\n",
    "\n",
    "print(\"\\n--- PSO Optimization Finished ---\")\n",
    "print(f\"Best cost (1 - val_accuracy) from PSO: {best_cost_pso:.4f}\")\n",
    "print(f\"This implies a best validation accuracy of: {1.0 - best_cost_pso:.4f}\")\n",
    "\n",
    "best_f1 = int(round(best_params_pso[0]))\n",
    "best_f2 = int(round(best_params_pso[1]))\n",
    "best_f3 = int(round(best_params_pso[2]))\n",
    "best_d1 = int(round(best_params_pso[3]))\n",
    "best_lr = float(best_params_pso[4])\n",
    "\n",
    "print(\"\\nFormatted Best Hyperparameters from PSO:\")\n",
    "print(f\"  Filters: Block1={best_f1}, Block2={best_f2}, Block3={best_f3}\")\n",
    "print(f\"  Dense Units: {best_d1}\")\n",
    "print(f\"  Learning Rate: {best_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ad6bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Model with PSO-Optimized Hyperparameters ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">924</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,834</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,668</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">39,270</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">118,405</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">796</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">356,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">796</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3184</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,598,870</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,008</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,300</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │           \u001b[38;5;34m924\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │           \u001b[38;5;34m132\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │         \u001b[38;5;34m9,834\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │           \u001b[38;5;34m132\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m66\u001b[0m)     │        \u001b[38;5;34m19,668\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m66\u001b[0m)     │           \u001b[38;5;34m264\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m66\u001b[0m)     │        \u001b[38;5;34m39,270\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m66\u001b[0m)     │           \u001b[38;5;34m264\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m66\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m66\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m199\u001b[0m)      │       \u001b[38;5;34m118,405\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m199\u001b[0m)      │           \u001b[38;5;34m796\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m199\u001b[0m)      │       \u001b[38;5;34m356,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m199\u001b[0m)      │           \u001b[38;5;34m796\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m199\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m199\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3184\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m502\u001b[0m)            │     \u001b[38;5;34m1,598,870\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m502\u001b[0m)            │         \u001b[38;5;34m2,008\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m502\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m50,300\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,198,271</span> (8.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,198,271\u001b[0m (8.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,196,075</span> (8.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,196,075\u001b[0m (8.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,196</span> (8.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,196\u001b[0m (8.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final PSO-Optimized Model ---\n",
      "Epoch 1/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.0751 - loss: 4.6238 - val_accuracy: 0.2241 - val_loss: 3.2172\n",
      "Epoch 2/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step - accuracy: 0.2092 - loss: 3.3299 - val_accuracy: 0.3150 - val_loss: 2.7284\n",
      "Epoch 3/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.3073 - loss: 2.7317 - val_accuracy: 0.3907 - val_loss: 2.3487\n",
      "Epoch 4/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.3699 - loss: 2.4084 - val_accuracy: 0.4225 - val_loss: 2.1868\n",
      "Epoch 5/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.4128 - loss: 2.2277 - val_accuracy: 0.4481 - val_loss: 2.0859\n",
      "Epoch 6/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.4494 - loss: 2.0440 - val_accuracy: 0.4640 - val_loss: 2.0096\n",
      "Epoch 7/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.4784 - loss: 1.8998 - val_accuracy: 0.4817 - val_loss: 1.9318\n",
      "Epoch 8/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.5007 - loss: 1.8052 - val_accuracy: 0.4997 - val_loss: 1.8818\n",
      "Epoch 9/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.5287 - loss: 1.6843 - val_accuracy: 0.5047 - val_loss: 1.8825\n",
      "Epoch 10/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.5502 - loss: 1.5941 - val_accuracy: 0.5137 - val_loss: 1.8315\n",
      "Epoch 11/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.5737 - loss: 1.4949 - val_accuracy: 0.5253 - val_loss: 1.7911\n",
      "Epoch 12/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.5915 - loss: 1.4238 - val_accuracy: 0.5366 - val_loss: 1.7554\n",
      "Epoch 13/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.6096 - loss: 1.3437 - val_accuracy: 0.5180 - val_loss: 1.8185\n",
      "Epoch 14/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.6241 - loss: 1.2903 - val_accuracy: 0.5522 - val_loss: 1.7260\n",
      "Epoch 15/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.6388 - loss: 1.2287 - val_accuracy: 0.5319 - val_loss: 1.8107\n",
      "Epoch 16/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.6558 - loss: 1.1625 - val_accuracy: 0.5606 - val_loss: 1.6873\n",
      "Epoch 17/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.6709 - loss: 1.1040 - val_accuracy: 0.5564 - val_loss: 1.7056\n",
      "Epoch 18/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.6778 - loss: 1.0760 - val_accuracy: 0.5545 - val_loss: 1.7432\n",
      "Epoch 19/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.6857 - loss: 1.0403 - val_accuracy: 0.5504 - val_loss: 1.7731\n",
      "Epoch 20/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.6971 - loss: 0.9963 - val_accuracy: 0.5490 - val_loss: 1.7729\n",
      "Epoch 21/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7089 - loss: 0.9534 - val_accuracy: 0.5565 - val_loss: 1.7389\n",
      "Epoch 22/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7245 - loss: 0.9209 - val_accuracy: 0.5697 - val_loss: 1.7098\n",
      "Epoch 23/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7205 - loss: 0.9102 - val_accuracy: 0.5621 - val_loss: 1.7616\n",
      "Epoch 24/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7285 - loss: 0.8751 - val_accuracy: 0.5629 - val_loss: 1.7600\n",
      "Epoch 25/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7429 - loss: 0.8275 - val_accuracy: 0.5698 - val_loss: 1.7515\n",
      "Epoch 26/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7532 - loss: 0.8032 - val_accuracy: 0.5723 - val_loss: 1.7506\n",
      "Epoch 27/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.7544 - loss: 0.7900 - val_accuracy: 0.5730 - val_loss: 1.7561\n",
      "Epoch 28/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.7580 - loss: 0.7713 - val_accuracy: 0.5666 - val_loss: 1.7917\n",
      "Epoch 29/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.7650 - loss: 0.7444 - val_accuracy: 0.5686 - val_loss: 1.7854\n",
      "Epoch 30/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.7700 - loss: 0.7367 - val_accuracy: 0.5819 - val_loss: 1.7467\n",
      "Epoch 31/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7794 - loss: 0.7077 - val_accuracy: 0.5718 - val_loss: 1.8329\n",
      "Epoch 32/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7741 - loss: 0.7129 - val_accuracy: 0.5790 - val_loss: 1.7832\n",
      "Epoch 33/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7824 - loss: 0.6824 - val_accuracy: 0.5656 - val_loss: 1.8696\n",
      "Epoch 34/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7920 - loss: 0.6609 - val_accuracy: 0.5725 - val_loss: 1.8105\n",
      "Epoch 35/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.7957 - loss: 0.6486 - val_accuracy: 0.5781 - val_loss: 1.7680\n",
      "Epoch 36/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.7980 - loss: 0.6402 - val_accuracy: 0.5752 - val_loss: 1.7913\n",
      "Epoch 37/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8002 - loss: 0.6289 - val_accuracy: 0.5777 - val_loss: 1.8315\n",
      "Epoch 38/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8005 - loss: 0.6151 - val_accuracy: 0.5840 - val_loss: 1.7837\n",
      "Epoch 39/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.7998 - loss: 0.6169 - val_accuracy: 0.5773 - val_loss: 1.8415\n",
      "Epoch 40/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8113 - loss: 0.5866 - val_accuracy: 0.5852 - val_loss: 1.8308\n",
      "Epoch 41/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8157 - loss: 0.5795 - val_accuracy: 0.5773 - val_loss: 1.8618\n",
      "Epoch 42/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8141 - loss: 0.5779 - val_accuracy: 0.5792 - val_loss: 1.8409\n",
      "Epoch 43/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8158 - loss: 0.5663 - val_accuracy: 0.5843 - val_loss: 1.8091\n",
      "Epoch 44/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8263 - loss: 0.5515 - val_accuracy: 0.5811 - val_loss: 1.8312\n",
      "Epoch 45/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8210 - loss: 0.5567 - val_accuracy: 0.5802 - val_loss: 1.8618\n",
      "Epoch 46/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8286 - loss: 0.5240 - val_accuracy: 0.5844 - val_loss: 1.8750\n",
      "Epoch 47/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8333 - loss: 0.5190 - val_accuracy: 0.5802 - val_loss: 1.8588\n",
      "Epoch 48/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8370 - loss: 0.5044 - val_accuracy: 0.5760 - val_loss: 1.9093\n",
      "Epoch 49/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8328 - loss: 0.5152 - val_accuracy: 0.5810 - val_loss: 1.8840\n",
      "Epoch 50/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8381 - loss: 0.5018 - val_accuracy: 0.5854 - val_loss: 1.8989\n",
      "Epoch 51/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8425 - loss: 0.4951 - val_accuracy: 0.5827 - val_loss: 1.9171\n",
      "Epoch 52/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8427 - loss: 0.4839 - val_accuracy: 0.5785 - val_loss: 1.9280\n",
      "Epoch 53/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8476 - loss: 0.4718 - val_accuracy: 0.5866 - val_loss: 1.8899\n",
      "Epoch 54/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8498 - loss: 0.4700 - val_accuracy: 0.5621 - val_loss: 2.0075\n",
      "Epoch 55/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8491 - loss: 0.4725 - val_accuracy: 0.5761 - val_loss: 1.9131\n",
      "Epoch 56/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8475 - loss: 0.4767 - val_accuracy: 0.5747 - val_loss: 1.9616\n",
      "Epoch 57/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8535 - loss: 0.4535 - val_accuracy: 0.5849 - val_loss: 1.9369\n",
      "Epoch 58/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8526 - loss: 0.4554 - val_accuracy: 0.5779 - val_loss: 1.9333\n",
      "Epoch 59/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8522 - loss: 0.4527 - val_accuracy: 0.5850 - val_loss: 1.9026\n",
      "Epoch 60/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8566 - loss: 0.4448 - val_accuracy: 0.5833 - val_loss: 1.9293\n",
      "Epoch 61/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8571 - loss: 0.4418 - val_accuracy: 0.5841 - val_loss: 1.9250\n",
      "Epoch 62/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8615 - loss: 0.4231 - val_accuracy: 0.5863 - val_loss: 1.9429\n",
      "Epoch 63/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 50ms/step - accuracy: 0.8628 - loss: 0.4259 - val_accuracy: 0.5875 - val_loss: 1.9328\n",
      "Epoch 64/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8659 - loss: 0.4163 - val_accuracy: 0.5775 - val_loss: 2.0051\n",
      "Epoch 65/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8651 - loss: 0.4209 - val_accuracy: 0.5757 - val_loss: 1.9918\n",
      "Epoch 66/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8682 - loss: 0.4071 - val_accuracy: 0.5888 - val_loss: 1.9371\n",
      "Epoch 67/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8688 - loss: 0.4084 - val_accuracy: 0.5875 - val_loss: 1.9416\n",
      "Epoch 68/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8682 - loss: 0.3979 - val_accuracy: 0.5819 - val_loss: 1.9666\n",
      "Epoch 69/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8700 - loss: 0.3972 - val_accuracy: 0.5860 - val_loss: 1.9660\n",
      "Epoch 70/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8715 - loss: 0.3858 - val_accuracy: 0.5845 - val_loss: 1.9627\n",
      "Epoch 71/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8727 - loss: 0.3836 - val_accuracy: 0.5840 - val_loss: 1.9804\n",
      "Epoch 72/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8712 - loss: 0.3890 - val_accuracy: 0.5765 - val_loss: 2.0392\n",
      "Epoch 73/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8765 - loss: 0.3797 - val_accuracy: 0.5796 - val_loss: 2.0393\n",
      "Epoch 74/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8803 - loss: 0.3656 - val_accuracy: 0.5874 - val_loss: 2.0294\n",
      "Epoch 75/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8785 - loss: 0.3673 - val_accuracy: 0.5888 - val_loss: 1.9692\n",
      "Epoch 76/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8751 - loss: 0.3813 - val_accuracy: 0.5890 - val_loss: 1.9627\n",
      "Epoch 77/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8782 - loss: 0.3738 - val_accuracy: 0.5896 - val_loss: 1.9689\n",
      "Epoch 78/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8782 - loss: 0.3720 - val_accuracy: 0.5924 - val_loss: 1.9673\n",
      "Epoch 79/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8839 - loss: 0.3558 - val_accuracy: 0.5894 - val_loss: 1.9865\n",
      "Epoch 80/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8830 - loss: 0.3596 - val_accuracy: 0.5895 - val_loss: 1.9683\n",
      "Epoch 81/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8840 - loss: 0.3562 - val_accuracy: 0.5881 - val_loss: 2.0080\n",
      "Epoch 82/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8846 - loss: 0.3553 - val_accuracy: 0.5888 - val_loss: 1.9809\n",
      "Epoch 83/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8867 - loss: 0.3474 - val_accuracy: 0.5873 - val_loss: 2.0338\n",
      "Epoch 84/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8868 - loss: 0.3406 - val_accuracy: 0.5892 - val_loss: 2.0042\n",
      "Epoch 85/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8873 - loss: 0.3408 - val_accuracy: 0.5914 - val_loss: 1.9848\n",
      "Epoch 86/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8909 - loss: 0.3348 - val_accuracy: 0.5904 - val_loss: 2.0008\n",
      "Epoch 87/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8900 - loss: 0.3354 - val_accuracy: 0.5909 - val_loss: 1.9841\n",
      "Epoch 88/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8876 - loss: 0.3412 - val_accuracy: 0.5896 - val_loss: 2.0264\n",
      "Epoch 89/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8904 - loss: 0.3287 - val_accuracy: 0.5880 - val_loss: 1.9925\n",
      "Epoch 90/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8948 - loss: 0.3225 - val_accuracy: 0.5943 - val_loss: 2.0045\n",
      "Epoch 91/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8945 - loss: 0.3195 - val_accuracy: 0.5859 - val_loss: 2.0119\n",
      "Epoch 92/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - accuracy: 0.8942 - loss: 0.3206 - val_accuracy: 0.5904 - val_loss: 1.9965\n",
      "Epoch 93/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8948 - loss: 0.3242 - val_accuracy: 0.5980 - val_loss: 1.9759\n",
      "Epoch 94/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - accuracy: 0.8994 - loss: 0.3091 - val_accuracy: 0.5935 - val_loss: 2.0135\n",
      "Epoch 95/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 53ms/step - accuracy: 0.8989 - loss: 0.3138 - val_accuracy: 0.5908 - val_loss: 2.0458\n",
      "Epoch 96/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - accuracy: 0.8987 - loss: 0.3122 - val_accuracy: 0.5971 - val_loss: 2.0042\n",
      "Epoch 97/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 53ms/step - accuracy: 0.8969 - loss: 0.3149 - val_accuracy: 0.5971 - val_loss: 2.0079\n",
      "Epoch 98/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.8984 - loss: 0.3057 - val_accuracy: 0.5911 - val_loss: 2.0060\n",
      "Epoch 99/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - accuracy: 0.8998 - loss: 0.3042 - val_accuracy: 0.5937 - val_loss: 2.0368\n",
      "Epoch 100/100\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - accuracy: 0.8974 - loss: 0.3049 - val_accuracy: 0.5955 - val_loss: 2.0341\n",
      "Restoring model weights from the end of the best epoch: 93.\n",
      "\n",
      "--- Evaluating PSO Optimized Model on Test Set ---\n",
      "\n",
      "PSO Optimized Test Accuracy: 0.5975\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "\n",
      "PSO Optimized Model Classification Report (Summary):\n",
      "Macro Avg F1-score: 0.5954\n",
      "Weighted Avg F1-score: 0.5954\n",
      "PSO Optimized Matthews Correlation Coefficient (MCC): 0.5935\n",
      "PSO Optimized Cohen's Kappa: 0.5934\n",
      "PSO Optimized Balanced Accuracy: 0.5975\n",
      "\n",
      "\n",
      "--- Comparison of Model Performance ---\n",
      "           Metric  Baseline Model  PSO Optimized Model\n",
      "         Accuracy          0.5897               0.5975\n",
      "Balanced Accuracy          0.5897               0.5975\n",
      "   Macro F1-score          0.5868               0.5954\n",
      "Weighted F1-score          0.5868               0.5954\n",
      "              MCC          0.5857               0.5935\n",
      "    Cohen's Kappa          0.5856               0.5934\n"
     ]
    }
   ],
   "source": [
    "# Define the Final PSO-Optimized CNN Model\n",
    "print(\"\\n--- Training Final Model with PSO-Optimized Hyperparameters ---\")\n",
    "\n",
    "final_model_pso = Sequential([\n",
    "    Input(shape=INPUT_SHAPE),\n",
    "    Conv2D(best_f1, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "    Conv2D(best_f1, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)), Dropout(0.2),\n",
    "    \n",
    "    Conv2D(best_f2, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "    Conv2D(best_f2, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)), Dropout(0.3),\n",
    "    \n",
    "    Conv2D(best_f3, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "    Conv2D(best_f3, (3, 3), padding='same', activation='relu'), BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)), Dropout(0.4),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(best_d1, activation='relu'), BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the Final Model\n",
    "final_optimizer = Adam(learning_rate=best_lr)\n",
    "final_model_pso.compile(\n",
    "    optimizer=final_optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model_pso.summary()\n",
    "\n",
    "# Train the Final Model\n",
    "print(\"\\n--- Training Final PSO-Optimized Model ---\")\n",
    "final_early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "history_final_pso = final_model_pso.fit(\n",
    "    X_train_full, y_train_full,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[final_early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Final PSO-Optimized Model\n",
    "print(\"\\n--- Evaluating PSO Optimized Model on Test Set ---\")\n",
    "loss_final_pso, accuracy_final_pso = final_model_pso.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nPSO Optimized Test Accuracy: {accuracy_final_pso:.4f}\")\n",
    "\n",
    "# Get full metrics\n",
    "y_pred_probs_pso = final_model_pso.predict(X_test)\n",
    "y_pred_pso = np.argmax(y_pred_probs_pso, axis=1)\n",
    "\n",
    "print(\"\\nPSO Optimized Model Classification Report (Summary):\")\n",
    "pso_report_dict = classification_report(y_test, y_pred_pso, output_dict=True)\n",
    "print(f\"Macro Avg F1-score: {pso_report_dict['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted Avg F1-score: {pso_report_dict['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "pso_mcc = matthews_corrcoef(y_test, y_pred_pso)\n",
    "pso_cohen_kappa = cohen_kappa_score(y_test, y_pred_pso)\n",
    "pso_balanced_accuracy = balanced_accuracy_score(y_test, y_pred_pso)\n",
    "\n",
    "print(f\"PSO Optimized Matthews Correlation Coefficient (MCC): {pso_mcc:.4f}\")\n",
    "print(f\"PSO Optimized Cohen's Kappa: {pso_cohen_kappa:.4f}\")\n",
    "print(f\"PSO Optimized Balanced Accuracy: {pso_balanced_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Final Comparison Table and Analysis\n",
    "print(\"\\n\\n--- Comparison of Model Performance ---\")\n",
    "\n",
    "comparison_data = {\n",
    "    \"Metric\": [\"Accuracy\", \"Balanced Accuracy\", \"Macro F1-score\", \"Weighted F1-score\", \"MCC\", \"Cohen's Kappa\"],\n",
    "    \"Baseline Model\": [accuracy_baseline, baseline_balanced_accuracy, baseline_macro_f1, baseline_weighted_f1, baseline_mcc, baseline_cohen_kappa],\n",
    "    \"PSO Optimized Model\": [accuracy_final_pso, pso_balanced_accuracy, pso_report_dict['macro avg']['f1-score'], pso_report_dict['weighted avg']['f1-score'], pso_mcc, pso_cohen_kappa]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582d919",
   "metadata": {},
   "source": [
    "# Analysis and Conclusion for CIFAR-100 Dataset\n",
    "\n",
    "The CIFAR-100 image classification task served as a classic computer vision problem to test the PSO methodology on a well-structured, balanced dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Baseline Model Performance\n",
    "\n",
    "- The baseline CNN, using a standard deep architecture and default hyperparameters, established a **strong performance benchmark**.\n",
    "- It achieved a **test accuracy of 58.97%**, which is significantly better than random chance (1% for 100 classes).\n",
    "- This result demonstrated that the architecture was well-suited for the task.\n",
    "- The central question became: _Could PSO refine the hyperparameters to achieve further gains?_\n",
    "\n",
    "---\n",
    "\n",
    "## PSO Optimization Results\n",
    "\n",
    "- The PSO algorithm explored **25 different hyperparameter combinations**.\n",
    "- Key findings include:\n",
    "  - The best-performing particle in the validation phase achieved **58.15%** accuracy.\n",
    "  - The optimal hyperparameters identified were:  \n",
    "    - **Filters:** 33, 66, 199  \n",
    "    - **Dense Units:** 502  \n",
    "    - **Learning Rate:** 0.001178\n",
    "\n",
    "- These values were **remarkably close** to the baseline configuration, indicating that:\n",
    "  > For this specific problem, the baseline architecture was already near an optimal point in the hyperparameter space.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Model Comparison\n",
    "\n",
    "| Metric            | Baseline Model | PSO Optimized Model | Change                |\n",
    "|-------------------|----------------|----------------------|------------------------|\n",
    "| Accuracy          | 0.5897         | 0.5975               | ✅ **+0.78%**          |\n",
    "| Balanced Accuracy | 0.5897         | 0.5975               | ✅ **+0.78%**          |\n",
    "| Macro F1-score    | 0.5868         | 0.5954               | ✅ **+0.86%**          |\n",
    "| MCC               | 0.5857         | 0.5935               | ✅ **+0.78%**          |\n",
    "| Cohen's Kappa     | 0.5856         | 0.5934               | ✅ **+0.78%**          |\n",
    "\n",
    "> The PSO-optimized model achieved a **test accuracy of 59.75%**, representing a small but consistent improvement across all evaluation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Conclusion\n",
    "\n",
    "This experiment demonstrates a **different, yet equally important**, outcome of hyperparameter optimization:\n",
    "\n",
    "- Unlike the **FMA audio task**, where PSO had to find a radically new solution to fix a failing model,\n",
    "- Here, PSO provided **fine-tuning refinement** on a problem where the **baseline was already strong**.\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "> PSO acted as an effective **local search**, confirming the quality of the original architecture and finding small tweaks that **polished performance**.\n",
    "\n",
    "This highlights the **versatility** of Particle Swarm Optimization:\n",
    "- 🔍 **Broad, exploratory search** on difficult problems\n",
    "- 🔧 **Fine-grained optimization** on well-behaved datasets\n",
    "\n",
    "**PSO delivers value in both scenarios.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
